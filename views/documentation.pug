doctype html 
html 
    head 
        title Documentation - Imagine Dungeons 
        link(rel="stylesheet" href="/static/css/style.css")
        link(rel="stylesheet" href="/static/css/documentation.css")
        link(rel="icon" type="image/x-icon" href="/static/images/favicon.ico")
        link(rel="stylesheet" href="https://pro.fontawesome.com/releases/v5.11.0/css/all.css")

        link(rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.57.0/codemirror.min.css")
        link(rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.57.0/theme/darcula.css")
        script(src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.57.0/codemirror.min.js")
        script(src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.57.0/mode/javascript/javascript.min.js")
    body 
        #loading
            .loading-animation 
            p Loading

        include includes/nav
        .documentation
            .side-menu
                .section
                    .section-icon
                        i.fad.fa-book-open
                    .section-links
                        a.h1(href="#introduction") Introduction
                .section
                    .section-icon
                        i.fad.fa-rocket
                    .section-links
                        a.h1(href="#getting-started") Getting Started
                .section
                    .section-icon
                        i.fad.fa-gamepad-alt
                    .section-links
                        a.h1(href="#game-editing") Game Editing
                        a.h2(href="#general-settings") General Settings
                        a.h2(href="#assets") Assets
                        a.h3(href="#types-and-properties") Types and Properties
                        a.h3(href="#adding-and-deleting-assets") Adding and Deleting Assets
                        a.h3(href="#changing-position-and-rotation") Changing Position and Rotation
                        a.h2(href="#game") Game
                        a.h3(href="#user-input") User Input
                        a.h3(href="#metrics") Metrics
                        a.h3(href="#rules") Rules
                .section
                    .section-icon
                        i.fad.fa-robot
                    .section-links
                        a.h1(href="#agent") Agent
                        a.h2(href="#reinforcement-learning-agent") What is a reinforcement learning agent?
                        a.h2(href="#available-libraries") Available Libraries
                        a.h2(href="#display") Display
                .section
                    .section-icon
                        i.fad.fa-lightbulb-on
                    .section-links
                        a.h1(href="#example") Example
                
            .container
                .section
                    h1#introduction Introduction
                    p Imagine Dungeons is a platform for creating and training agents to play 3D dungeon games using reinforcement learning. With Imagine Dungeons, you can create games or use existing ones and then train your agents to play them. The platform provides various tools and libraries to simplify the process.

                    p Through Imagine Dungeons, you will gain experience creating games, building agents to play those games, and training those agents using reinforcement learning. This platform provides a user-friendly interface, enabling you to develop and train agents using popular deep-learning frameworks such as TensorFlow.js. Whether you're new to the field or have experience with machine learning, Imagine Dungeons can help you create and train agents that can learn to play complex games and make decisions in complex environments.

                .section
                    h1#getting-started Getting Started
                    p To get started with Imagine Dungeons, you can follow these simple steps:
                    ol 
                        li Sign up for an account on the Imagine Dungeons website.
                        li Create your own game using the platform's game engine.
                        li Create your agent training script using the available libraries.
                        li Evaluate the performance of your agent and fine-tune it as needed.
                        li Share your agent with the community or use it for your projects.
                    p With Imagine Dungeons, you can create and train agents that can learn to play complex games and make intelligent decisions based on their environment. The platform is easy to use and flexible, making it an ideal tool for developers and researchers who want to explore the field of reinforcement learning and train intelligent agents.
                
                .section 
                    h1#game-editing Game Editing
                    p You'll be directed to the game editing page when you create a new game. You can also access this page by clicking on the [edit icon] of the game on the Your Games page. You can customize a 3D dungeon game on this page to match your reinforcement learning plans.

                    h2#general-settings General Settings
                    p Each game has several general settings:

                    ul 
                        li 
                            b Name: 
                            | This name is used only for reference within the application.
                        li 
                            b Map size: 
                            | This setting indicates the size of the map in the x and z directions. Changing these values adjusts the map size, which you can see in the layout tab. The map will adjust from the center, so increasing the x or z map size increases both directions in that axis.

                    h2#assets Assets
                    p On the game editing page, you'll find a variety of assets available for your dungeon-themed game. Each asset added to the scene will have a unique name you cannot modify, consisting of the original model name with an increasing index number.

                    h3#types-and-properties Types and Properties
                    p Different types of assets behave differently in the game. Here are the available asset types and their properties:

                    ul 
                        li 
                            b Modular: 
                            | These assets construct the game's map and include building pieces. This type of asset doesn't have any customizable properties. Even though you can't customize them, the player can interact with them. This type of object is considered a solid object, so the player can't cross it. For the arch_bars and end_door, the player can interact with them. When interacting with arch_bars (by pressing 'e' and being close to it), it will move down. When interacting with end_door, the game will end, and the player will win.
                        li 
                            b Player: 
                            | You can choose from a few players to add to the scene, but only one player can exist. The player is a requirement for the game to run because the camera follows the player. The player has life, an initial attack weapon, and an initial defense weapon as customizable properties. The options for attack and defense weapons come from the weapons added to the project. If you don't choose an initial attack weapon, the player won't be able to attack until it gets a weapon. If you select an initial attack/defense weapon, the object will keep its position on the map when on the editing page, but when the game starts, the player will wear them. The player can wear both attack and defense weapons at the same time.
                        li 
                            b Monster: 
                            | You can choose from various options of monsters. The monsters have life, strength, and attack range as customizable properties. The attack range represents the number of units the player needs to be from the monster for it to start following the player to attack.
                        li 
                            b Light: 
                            | This type of asset doesn't have any customizable properties. It provides a Three.js point light at the location of the light source.
                        li 
                            b Decor: 
                            | These objects are related to the game's theme and can be added to the scene as solid objects.
                        li 
                            b Weapon: 
                            | The weapon is divided into two types: attack and defense. Both types have a strength property that represents their power over enemies.
                    
                    p To view an asset's properties, click on the asset in the layout scene or project tab. The asset's properties will then appear in the properties tab.

                    h3#adding-and-deleting-assets Adding and Deleting Assets

                    p To add an asset to your game, navigate to the assets tab and select the section corresponding to the asset type you want to add. Next, click on the asset you want to include, and it will appear in the center of the game at position (0,0). You can then find the asset listed in the project tab with its unique name.
                    p To remove an asset from your game, locate it in the project tab and click on its trash icon. This will delete the asset from your game and will no longer be visible in the scene. Note that once an asset is removed, you cannot recover it, so keep saving your project before making any significant changes.

                    h3#changing-position-and-rotation Changing Position and Rotation

                    p To change the assets' position, you can drag the object in the scene or choose to only change its position in a single axis by pressing G and dragging that axis vector. Note that the asset can only move in the x and z-axis. The y-axis is fixed. Also, the object cannot move outside the map, and the game engine will round its position to the nearest half unit, so moving in a discrete space allows you to better position the elements in the scene.
                    p To change an asset's rotation, you can drag the green circle that appears beneath the asset to the desired direction. The asset's rotation is executed only in 90-degree increments on the y-axis. If the green circle does not appear, press R to make it appear.

                    h2#game Game
                    p To play the game you created, click the “play” button. This will take you to the play page, where you can interact with the game and see various metrics.

                    h3#user-input User Input
                    p To control the player, you can use the keyboard. The following commands are available:

                    ul 
                        li 
                            b 'w': 
                            | Move forward
                        li  
                            b 'a': 
                            | Turn left
                        li  
                            b 'd': 
                            | turn right
                        li  
                            b 'e': 
                            | Interact with objects in the game
                        li  
                            b Space: 
                            | Attack

                    h3#metrics Metrics 
                    p While playing the game, it will display various metrics to help you keep track of your progress. These metrics include:

                    ul 
                        li 
                            b Time: 
                            | The amount of time elapsed since the start of the game
                        li  
                            b Life: 
                            | The current life of the player
                        li  
                            b XP: 
                            | Experience points collected by the player
                        li  
                            b Level: 
                            | The current level of the player
                        li  
                            b Attack: 
                            | The current attack strength of the player
                        li  
                            b Defense: 
                            | The current defense strength of the player

                    h3#rules Rules 

                    h4 Player Attacking

                    ul.keep_style
                        li After initiating an attack, the player waits for the attack animation to finish before executing any other action.
                        li When the attack animation is complete, the player damages the monster in front of them. If multiple monsters are present, only the first monster will be affected.
                        li The damage inflicted on the monster equals the player's attack strength.

                    h4 Monster Attacking

                    ul.keep_style
                        li The monster will follow the player until they intersect, at which point it will attack.
                        li  After each attack, if the monster is still intersecting with the player, the player's life will decrease by the monster's attack strength.
                        li  If the player is wearing a defense weapon, the strength of the attack will be reduced by the defense strength of the player's weapon. If the weapon strength is greater than 90% of the monster's attack strength, it will reduce the attack to 10% of the monster's strength.
        
                    h4 Interacting

                    p Particular objects in the scene allow the player to interact with them.

                    ul.keep_style
                        li arch_bars: Interacting with this object causes the bars to move down until the player can cross. The player needs to continue interacting until the bars are completely lowered.
                        li end_door: Interacting with this object immediately ends the game.
                        li  Weapons: Interacting with a weapon causes the player to equip it. The two weapons will swap places if the player already holds a weapon.

                    h4 Ending game
                    p There are three ways to end the game:

                    h5 Winning

                    ul.keep_style
                        li Find the exit: If an end_door object is present in the scene, the player can interact with it to end the game and win.
                        li Kill all monsters: If there is no end_door object in the scene, the player can win by killing all the monsters.
        
                    h5 Losing
                    ul.keep_style
                        li Player death: If the player's life decreases to 0, the game ends, and the monsters start dancing.

                    p If there is no end_door object and no monsters in the scene, the game will start and end immediately without allowing the player to take any actions. After the game ends, the player cannot take any further actions.

                .section
                    h1#agent Agent

                    p On the agent page, you can create artificial intelligence (AI) agents to play the games you created on the platform.

                    h2#reinforcement-learning-agent What is a reinforcement learning agent?
                    p A reinforcement learning agent is an AI agent that learns to play a game through trial and error. The agent interacts with the game environment and receives rewards or penalties based on its actions. The agent's goal is to learn to take actions that maximize its long-term reward, typically defined as a cumulative sum of the rewards obtained over time.
                    p Reinforcement learning agents use a trial-and-error approach to explore different actions and learn from the feedback received from the environment. They update their policy or decision-making strategy based on the rewards obtained, so they can learn to make better decisions over time. As the agent learns to play the game, it becomes increasingly effective and efficient and may even outperform human players in some cases.
                    p In Imagine Dungeons, you can create a reinforcement learning agent using JavaScript and the available libraries. You can train your agent to play the game by defining its policy and using the built-in functions to interact with the game environment. You can also test your agent's performance and fine-tune its policy to improve its gameplay.

                    h2#available-libraries Available Libraries

                    p The following libraries are available to use in Imagine Dungeons to develop and train reinforcement learning agents:

                    h4 GameEnv
                    p This library acts as a bridge between your code and the game. It provides the reset and step functions, and you can set the metric weights used in the reward function, which calculates the reward at every step.

                    h4 DBManager
                    p This class enables you to save and retrieve JSON objects from the database to avoid losing data during agent training. For example, you can save your model weights in a JSON format every few episodes to ensure you do not lose any progress. Note that the database has a size limit of 16 MB.

                    h4 Memory
                    p To train reinforcement learning agents, it is crucial to save previous episodes so that the agent can be retrained with them in the future and not forget what it has learned. You can use this class to store this information while running your code.

                    h4 TensorFlow.js
                    p TensorFlow.js is a popular library for creating neural networks and reinforcement learning agents. In Imagine Dungeons, TensorFlow.js can utilize GPU resources, resulting in similar or even better processing times than when working with TensorFlow in a Python environment.

                    h2#display Display 
                    p When the code is running, you will see four tabs on the right side of the screen:
                    ul 
                        li 
                            b Game: 
                            | This tab displays the game that the agent is currently playing. You can see the actions taken by the agent and the changes in the game environment in real time.
                        li 
                            b Render: 
                            | This tab displays the observation space that the agent is receiving. The observation space contains information about the state of the game environment and is used by the agent to make decisions.
                        li 
                            b Plot: 
                            | This tab displays a graph of the rewards earned by the agent over time, since the start of the training process. This can help you visualize how the agent's performance improves over time.
                        li 
                            b Metrics: 
                            | This tab displays the current metrics of the game, such as the ***current episode, the total number of episodes, the average reward, and the current score***. This information can help understand the game's progress and the agent's performance.


                .section
                    h1#example Example

                    h2 How to create a game?
                    p TODO
                    h2 How to create an agent?
                    p IN PROGRESS 

                    p Reinforcement learning (RL) is a subfield of machine learning that involves an agent learning how to make decisions by interacting with an environment. The goal of the agent is to learn a policy that maximizes a reward signal provided by the environment. The agent learns by trial and error, receiving feedback in the form of a reward signal from the environment based on the actions it takes. The objective is to maximize the cumulative reward over time.

                    p The code defines a DQNAgent class that initializes the Q network and the target Q network. The Q network is a convolutional neural network that takes the current state of the game as input and outputs the Q-values for each possible action. The Q-values represent the expected future reward for each action, given the current state. The target Q network is used for computing the target Q-values during training. It is a copy of the Q network that is updated less frequently and is used to stabilize the training process.

                    p The DQNAgent class also defines the hyperparameters and functions for selecting actions, storing transitions, and updating the Q network. The getAction function selects the next action based on an epsilon-greedy policy, which selects a random action with probability epsilon and selects the action with the highest Q-value with probability 1 - epsilon. The storeTransition function stores the current transition (state, action, reward, next state, done) in a replay memory. The updateQNetwork function updates the Q network using a minibatch of transitions sampled randomly from the replay memory. It computes the target Q-values using the target Q network and the Bellman equation and trains the Q network using the mean squared error loss.

                    p The main function creates an instance of the GameEnv class and initializes the environment. It then creates an instance of the DQNAgent class and trains the agent using the environment. The reward_weights dictionary is used to specify the weights for the different components of the reward function. The agent learns to maximize the cumulative reward by taking actions that lead to higher rewards and avoiding actions that lead to lower rewards. The main function iterates over episodes, where each episode involves the agent playing the game until it reaches a terminal state (game over or game win).
        
                    .code_textarea
                        textarea(id="code").
                            // Define the DQN agent
                            class DQNAgent {
                            constructor(numActions, inputShape) {
                                this.numActions = numActions;
                                this.inputShape = inputShape;

                                // Define the Q network
                                this.qNetwork = tf.sequential({
                                layers: [
                                    tf.layers.conv2d({inputShape: inputShape, filters: 32, kernelSize: 8, strides: 4, activation: 'relu'}),
                                    tf.layers.conv2d({filters: 64, kernelSize: 4, strides: 2, activation: 'relu'}),
                                    tf.layers.conv2d({filters: 64, kernelSize: 3, strides: 1, activation: 'relu'}),
                                    tf.layers.flatten(),
                                    tf.layers.dense({units: 512, activation: 'relu'}),
                                    tf.layers.dense({units: numActions})
                                ]
                                });

                                // Define the target Q network
                                this.targetQNetwork = tf.sequential({
                                layers: [
                                    tf.layers.conv2d({inputShape: inputShape, filters: 32, kernelSize: 8, strides: 4, activation: 'relu'}),
                                    tf.layers.conv2d({filters: 64, kernelSize: 4, strides: 2, activation: 'relu'}),
                                    tf.layers.conv2d({filters: 64, kernelSize: 3, strides: 1, activation: 'relu'}),
                                    tf.layers.flatten(),
                                    tf.layers.dense({units: 512, activation: 'relu'}),
                                    tf.layers.dense({units: numActions})
                                ]
                                });

                                // Compile the Q network
                                this.qNetwork.compile({optimizer: tf.train.adam(), loss: 'meanSquaredError'});

                                // Define the hyperparameters
                                this.gamma = 0.99;
                                this.epsilon = 1.0;
                                this.epsilonMin = 0.01;
                                this.epsilonDecay = 0.9995;
                                this.batchSize = 32;
                                this.memory = [];
                                this.memoryCapacity = 10000;
                                this.updateTargetFrequency = 1000;
                                this.stepCounter = 0;
                            }

                            // Get an action from the Q network
                            getAction(state) {
                                if (Math.random() < this.epsilon) {
                                return Math.floor(Math.random() * this.numActions);
                                } else {
                                const qValues = this.qNetwork.predict(tf.tensor(state).reshape([1, this.inputShape[0], this.inputShape[1], 1]).div(255));
                                return qValues.argMax().dataSync()[0];
                                }
                            }

                            // Store a transition in the memory
                            storeTransition(transition) {
                                this.memory[this.memoryIndex] = transition;
                                this.memoryIndex = (this.memoryIndex + 1) % this.memoryCapacity;
                            }

                            // Update the Q network
                            updateQNetwork() {
                                if (this.memory.length < this.batchSize) {
                                return;
                                }

                                // Sample a batch from the memory
                                const indices = tf.tidy(() => tf.randomUniform([this.batchSize], 0, this.memory.length).cast('int32'));
                                const batch = tf.tidy(() => {
                                const stateBatch = [];
                                const actionBatch = [];
                                const rewardBatch = [];
                                const nextStateBatch = [];
                                const doneBatch = [];

                                indices.arraySync().forEach(index => {
                                    const [state, action, reward, nextState, done] = this.memory[index];
                                    stateBatch.push(tf.tensor(state).reshape([1, this.inputShape[0], this.inputShape[1], 1]).div(255));
                                    actionBatch.push(action);
                                    rewardBatch.push(reward);
                                    nextStateBatch.push(tf.tensor(nextState).reshape([1, this.inputShape[0], this.inputShape[1], 1]).div(255));
                                    doneBatch.push(done);
                                });

                                return [tf.concat(stateBatch), tf.tensor1d(actionBatch, 'int32'), tf.tensor1d(rewardBatch), tf.concat(nextStateBatch), tf.tensor1d(doneBatch, 'bool')];
                                });

                                // Compute the target Q values
                                const targetQValues = tf.tidy(() => {
                                const qValues = this.qNetwork.predict(batch[3]);
                                const maxQValues = this.targetQNetwork.predict(batch[3]).max(1);
                                const targetQValues = batch[2].add(maxQValues.mul(this.gamma).mul(tf.logicalNot(batch[4]).cast('float32')));
                                return tf.oneHot(batch[1], this.numActions).mul(targetQValues.reshape([-1, 1]));
                                });

                                // Train the Q network
                                this.qNetwork.trainOnBatch(batch[0], targetQValues);

                                // Update the target Q network
                                if (this.stepCounter % this.updateTargetFrequency === 0) {
                                this.targetQNetwork.setWeights(this.qNetwork.getWeights());
                                }

                                // Update the epsilon
                                this.epsilon = Math.max(this.epsilonMin, this.epsilon * this.epsilonDecay);
                                
                                // Increment the step counter
                                this.stepCounter++;
                            }
                            }

                            // Define the main function
                            async function main() {
                            // define weights for the reward function
                            const reward_weights = {
                                "health": 0,
                                "xp": 0,
                                "level": 0,
                                "defense": 0,
                                "attack": 0,
                                "time": -0.005,
                                "game_over": 0,
                                "game_win": 100
                            }
                            // Create the environment
                            const env = new GameEnv(width = 100, height = 75, weights = reward_weights)

                            // Initialize the agent
                            const agent = new DQNAgent(env.action_space, env.observation_space);
                            
                            // create a new instance of DBManager
                            const db = new DBManager();

                            // try to retrieve the saved network from the database
                            let savedNetwork = await db.getObject();

                            if (savedNetwork) {
                                console.log("Using saved network...");
                                agent.qNetwork.setWeights(savedNetwork.qWeights);
                                agent.targetQNetwork.setWeights(savedNetwork.targetQWeights);
                                console.log("Correctly compiled");
                            }
                            

                            // Run the training loop
                            for (let episode = 0; episode < 1000; episode++) {
                                
                                let [state] = await env.reset();
                                
                                let done = false;
                                let totalReward = 0;
                                while (!done) {
                                    // Get an action from the agent
                                
                                    const action = agent.getAction(state);
                                    // Take the action in the environment
                                    const [nextState, reward, isDone] = await env.step(action);
                                    env.render();

                                    // Store the transition in the memory
                                    agent.storeTransition([state, action, reward, nextState, isDone]);

                                    // Update the Q network
                                    agent.updateQNetwork();

                                    // Update the state and reward
                                    state = nextState;
                                    totalReward += reward;
                                    done = isDone;
                                }
                                
                                // save the network to the database
                                await db.saveObject({
                                qWeights: agent.qNetwork.getWeights(),
                                targetQWeights: agent.targetQNetwork.getWeights()
                                });

                                console.log(`Episode ${episode}: total reward = ${totalReward}`);
                            }
                            }

                            // Run the main function
                            main();
        include includes/footer 
            
style. 
    .CodeMirror {
        height: auto !important;
    }

script.
    window.addEventListener("load", function() {
        const loading = document.querySelector("#loading");
        loading.style.display = "none";
    });
    window.editor = CodeMirror.fromTextArea(document.getElementById("code"), {
        mode: "javascript",
        lineNumbers: true,
        theme: 'darcula',
        autocorrect: true,
        indentUnit: 2
    });

    window.editor.on('change', (instance, changeObj) => {
        document.getElementById("code").value = window.editor.getValue()
    })
